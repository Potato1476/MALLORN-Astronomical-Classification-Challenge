{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14034160,"sourceType":"datasetVersion","datasetId":8936916},{"sourceId":14199657,"sourceType":"datasetVersion","datasetId":9055673}],"dockerImageVersionId":31193,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 0\n!pip install -q tsfresh\n!pip install -q lightgbm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-17T19:43:51.044761Z","iopub.execute_input":"2025-12-17T19:43:51.045065Z","iopub.status.idle":"2025-12-17T19:43:57.472816Z","shell.execute_reply.started":"2025-12-17T19:43:51.045042Z","shell.execute_reply":"2025-12-17T19:43:57.471735Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 1 – Imports\nimport os\nimport numpy as np\nimport pandas as pd\n\nfrom tsfresh import extract_features, select_features\nfrom tsfresh.feature_extraction import EfficientFCParameters\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, log_loss, f1_score\nfrom sklearn.preprocessing import StandardScaler # Dòng mới\nfrom sklearn.linear_model import LogisticRegression # Dòng mới\nimport xgboost as xgb\nimport lightgbm as lgb # Dòng mới","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T19:43:57.474558Z","iopub.execute_input":"2025-12-17T19:43:57.474820Z","iopub.status.idle":"2025-12-17T19:43:57.480400Z","shell.execute_reply.started":"2025-12-17T19:43:57.474788Z","shell.execute_reply":"2025-12-17T19:43:57.479740Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 2 – Locate DATA_DIR\ncandidate_dirs = [\n    \"/kaggle/input/dataset-macc/dataset_mallorn-astronomical-classification-challenge\"\n]\n\nDATA_DIR = None\nfor d in candidate_dirs:\n    if os.path.exists(os.path.join(d, \"train_log.csv\")):\n        DATA_DIR = d\n        break\n\nif DATA_DIR is None:\n    raise FileNotFoundError(\n        \"Could not find train_log.csv. \"\n        \"Check /kaggle/input and adjust DATA_DIR accordingly.\"\n    )\n\nprint(\"Using DATA_DIR:\", DATA_DIR)\nprint(\"Files:\", os.listdir(DATA_DIR))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T19:43:57.481243Z","iopub.execute_input":"2025-12-17T19:43:57.481491Z","iopub.status.idle":"2025-12-17T19:43:57.500691Z","shell.execute_reply.started":"2025-12-17T19:43:57.481476Z","shell.execute_reply":"2025-12-17T19:43:57.499872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 3 – Load metadata\ntrain_log = pd.read_csv(os.path.join(DATA_DIR, \"train_log.csv\"))\ntest_log  = pd.read_csv(os.path.join(DATA_DIR, \"test_log.csv\"))\nsample_sub = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n\nprint(\"train_log:\", train_log.shape)\nprint(\"test_log:\", test_log.shape)\nprint(\"sample_submission:\", sample_sub.shape)\n\nprint(\"\\nTarget distribution:\")\nprint(train_log[\"target\"].value_counts(normalize=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T19:43:57.502524Z","iopub.execute_input":"2025-12-17T19:43:57.502784Z","iopub.status.idle":"2025-12-17T19:43:57.553177Z","shell.execute_reply.started":"2025-12-17T19:43:57.502767Z","shell.execute_reply":"2025-12-17T19:43:57.552534Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4 – Lightcurve feature function (UPDATED: cadence + peak + noise-aware)\ndef make_features(\n    lc_df: pd.DataFrame,\n    clip_flux: bool = True,\n    clip_low: float = -5.0,\n    clip_high: float = 5.0,\n    drop_fluxerr_quantile: float = 0.99,\n) -> pd.DataFrame:\n    \"\"\"\n    From lightcurves:\n        object_id, Time (MJD), Flux, Flux_err, Filter\n    -> one row per object_id with per-filter + global stats.\n\n    New additions (from EDA approach):\n      - Drop noisiest 1% Flux_err per filter\n      - Flux clipping for robust peak/shape\n      - Cadence features: max_gap, gap_std, obs_density\n      - Peak features: peak_flux/time, time_to_peak, rise/decay slopes, asymmetry\n      - Shape/noise: n_local_peaks, auc_pos/abs, smoothness, snr_peak\n      - Cross-filter: peak_time_diff, peak_flux_ratio\n    \"\"\"\n    lc_df = lc_df.drop_duplicates().copy()\n    lc_df = lc_df.replace([np.inf, -np.inf], np.nan)\n    lc_df = lc_df.dropna(subset=[\"object_id\", \"Time (MJD)\", \"Flux\", \"Flux_err\", \"Filter\"])\n\n    # Drop top 1% Flux_err per filter (noise-aware)\n    if drop_fluxerr_quantile is not None:\n        thr = lc_df.groupby(\"Filter\")[\"Flux_err\"].transform(lambda s: s.quantile(drop_fluxerr_quantile))\n        lc_df = lc_df[lc_df[\"Flux_err\"] <= thr]\n\n    # Clip heavy tails for robust peak/shape\n    lc_df[\"Flux_clip\"] = lc_df[\"Flux\"].clip(clip_low, clip_high) if clip_flux else lc_df[\"Flux\"]\n\n    # Per-measurement helpers\n    lc_df[\"snr\"] = lc_df[\"Flux\"] / lc_df[\"Flux_err\"].replace(0, np.nan)\n    lc_df[\"snr\"] = lc_df[\"snr\"].fillna(0.0)\n    lc_df[\"snr_clip\"] = lc_df[\"Flux_clip\"] / lc_df[\"Flux_err\"].replace(0, np.nan)\n    lc_df[\"snr_clip\"] = lc_df[\"snr_clip\"].fillna(0.0)\n    lc_df[\"is_pos\"] = (lc_df[\"Flux\"] > 0).astype(float)\n\n    # --- 1) Per-band aggregates ---\n    agg = lc_df.groupby([\"object_id\", \"Filter\"]).agg(\n        flux_mean=(\"Flux\", \"mean\"),\n        flux_std=(\"Flux\", \"std\"),\n        flux_min=(\"Flux\", \"min\"),\n        flux_max=(\"Flux\", \"max\"),\n        flux_median=(\"Flux\", \"median\"),\n        flux_count=(\"Flux\", \"count\"),\n        flux_abs_mean=(\"Flux\", lambda x: float(np.mean(np.abs(x)))),\n\n        flux_clip_mean=(\"Flux_clip\", \"mean\"),\n        flux_clip_std=(\"Flux_clip\", \"std\"),\n        flux_clip_min=(\"Flux_clip\", \"min\"),\n        flux_clip_max=(\"Flux_clip\", \"max\"),\n\n        time_min=(\"Time (MJD)\", \"min\"),\n        time_max=(\"Time (MJD)\", \"max\"),\n\n        err_mean=(\"Flux_err\", \"mean\"),\n        err_std=(\"Flux_err\", \"std\"),\n\n        snr_mean=(\"snr\", \"mean\"),\n        snr_std=(\"snr\", \"std\"),\n        snr_max=(\"snr\", \"max\"),\n        snr_clip_max=(\"snr_clip\", \"max\"),\n\n        pos_frac=(\"is_pos\", \"mean\"),\n    ).reset_index()\n\n    agg[\"time_range\"] = agg[\"time_max\"] - agg[\"time_min\"]\n    agg[\"flux_amp\"] = agg[\"flux_max\"] - agg[\"flux_min\"]\n    agg[\"flux_clip_amp\"] = agg[\"flux_clip_max\"] - agg[\"flux_clip_min\"]\n    agg[\"obs_rate\"] = agg[\"flux_count\"] / agg[\"time_range\"].replace(0, np.nan)\n    agg[\"obs_rate\"] = agg[\"obs_rate\"].fillna(0.0)\n\n    # --- 2) Extra per-band (cadence + peak + shape) ---\n    def _extra_band_feats(group: pd.DataFrame) -> pd.Series:\n        t = group[\"Time (MJD)\"].values\n        f = group[\"Flux_clip\"].values\n        snr = group[\"snr\"].values\n\n        if len(t) > 0:\n            order = np.argsort(t)\n            t = t[order]\n            f = f[order]\n            snr = snr[order]\n\n        # cadence / slopes\n        slope_max = slope_min = slope_mean = 0.0\n        dt_mean = dt_max = dt_std = 0.0\n        obs_density = 0.0\n\n        if len(t) > 1:\n            dt = np.diff(t)\n            valid = dt > 0\n            if np.any(valid):\n                df = np.diff(f)\n                slopes = df[valid] / dt[valid]\n                slope_max = float(np.max(slopes))\n                slope_min = float(np.min(slopes))\n                slope_mean = float(np.mean(slopes))\n                dt_mean = float(np.mean(dt[valid]))\n                dt_max  = float(np.max(dt[valid]))\n                dt_std  = float(np.std(dt[valid]))\n\n            duration = float(t[-1] - t[0])\n            obs_density = float(len(t) / duration) if duration > 0 else 0.0\n\n        # robust amplitude\n        if len(f) > 0:\n            flux_p10 = float(np.percentile(f, 10))\n            flux_p90 = float(np.percentile(f, 90))\n            flux_p90_p10 = float(flux_p90 - flux_p10)\n        else:\n            flux_p10 = flux_p90 = flux_p90_p10 = 0.0\n\n        # peak features\n        peak_flux = peak_time = time_to_peak = 0.0\n        rise_slope = decay_slope = asymmetry = 0.0\n        snr_peak = 0.0\n\n        if len(f) > 0:\n            i_peak = int(np.argmax(f))\n            peak_flux = float(f[i_peak])\n            peak_time = float(t[i_peak]) if len(t) > 0 else 0.0\n            time_to_peak = float(peak_time - t[0]) if len(t) > 0 else 0.0\n\n            def lin_slope(tt, ff):\n                if len(tt) < 2: return 0.0\n                if float(tt[-1] - tt[0]) == 0.0: return 0.0\n                return float(np.polyfit(tt, ff, 1)[0])\n\n            rise_slope = lin_slope(t[:i_peak+1], f[:i_peak+1])\n            decay_slope = lin_slope(t[i_peak:], f[i_peak:])\n\n            rise_time = float(t[i_peak] - t[0]) if len(t) > 0 else 0.0\n            decay_time = float(t[-1] - t[i_peak]) if len(t) > 0 else 0.0\n            asymmetry = float(rise_time / (decay_time + 1e-6))\n\n            if len(snr) > i_peak:\n                snr_peak = float(snr[i_peak])\n\n        # local peaks (simple + robust threshold)\n        n_local_peaks = 0\n        if len(f) >= 3:\n            loc = (f[1:-1] > f[:-2]) & (f[1:-1] > f[2:])\n            prom = f[1:-1] - np.maximum(f[:-2], f[2:])\n            thr = 0.5 * float(np.std(f)) if float(np.std(f)) > 0 else 0.0\n            n_local_peaks = int(np.sum(loc & (prom > thr)))\n\n        # AUC and smoothness\n        auc_pos = auc_abs = smoothness = 0.0\n        if len(f) >= 2:\n            auc_pos = float(np.trapz(np.clip(f, 0, None), t))\n            auc_abs = float(np.trapz(np.abs(f), t))\n            if len(f) >= 3:\n                smoothness = float(np.mean(np.abs(np.diff(f, 2)))) / (flux_p90_p10 + 1e-6)\n\n        return pd.Series({\n            \"slope_max\": slope_max,\n            \"slope_min\": slope_min,\n            \"slope_mean\": slope_mean,\n            \"dt_mean\": dt_mean,\n            \"max_gap\": dt_max,\n            \"gap_std\": dt_std,\n            \"obs_density\": obs_density,\n\n            \"flux_p10\": flux_p10,\n            \"flux_p90\": flux_p90,\n            \"flux_p90_p10\": flux_p90_p10,\n\n            \"peak_flux\": peak_flux,\n            \"peak_time\": peak_time,\n            \"time_to_peak\": time_to_peak,\n            \"rise_slope\": rise_slope,\n            \"decay_slope\": decay_slope,\n            \"asymmetry\": asymmetry,\n            \"snr_peak\": snr_peak,\n\n            \"n_local_peaks\": n_local_peaks,\n            \"auc_pos\": auc_pos,\n            \"auc_abs\": auc_abs,\n            \"smoothness\": smoothness,\n        })\n\n    extra = (\n        lc_df.groupby([\"object_id\", \"Filter\"])[[\"Time (MJD)\", \"Flux_clip\", \"snr\"]]\n        .apply(_extra_band_feats)\n        .reset_index()\n    )\n    agg = agg.merge(extra, on=[\"object_id\", \"Filter\"], how=\"left\")\n\n    # Pivot to wide\n    feat_pivot = agg.pivot(\n        index=\"object_id\",\n        columns=\"Filter\",\n        values=[\n            \"flux_mean\", \"flux_std\", \"flux_min\", \"flux_max\", \"flux_median\", \"flux_count\", \"flux_abs_mean\",\n            \"flux_clip_mean\", \"flux_clip_std\", \"flux_clip_min\", \"flux_clip_max\",\n            \"time_min\", \"time_max\", \"time_range\",\n            \"flux_amp\", \"flux_clip_amp\", \"obs_rate\",\n            \"err_mean\", \"err_std\",\n            \"snr_mean\", \"snr_std\", \"snr_max\", \"snr_clip_max\",\n            \"pos_frac\",\n            \"slope_max\", \"slope_min\", \"slope_mean\",\n            \"dt_mean\", \"max_gap\", \"gap_std\", \"obs_density\",\n            \"flux_p10\", \"flux_p90\", \"flux_p90_p10\",\n            \"peak_flux\", \"peak_time\", \"time_to_peak\", \"rise_slope\", \"decay_slope\", \"asymmetry\", \"snr_peak\",\n            \"n_local_peaks\", \"auc_pos\", \"auc_abs\", \"smoothness\",\n        ],\n    )\n    feat_pivot.columns = [f\"{stat}_f{band}\" for stat, band in feat_pivot.columns]\n    feat_pivot = feat_pivot.reset_index()\n\n    # --- 3) Global stats ---\n    g = lc_df.groupby(\"object_id\").agg(\n        flux_mean_all=(\"Flux\", \"mean\"),\n        flux_std_all=(\"Flux\", \"std\"),\n        flux_min_all=(\"Flux\", \"min\"),\n        flux_max_all=(\"Flux\", \"max\"),\n        flux_median_all=(\"Flux\", \"median\"),\n        flux_count_all=(\"Flux\", \"count\"),\n        flux_abs_mean_all=(\"Flux\", lambda x: float(np.mean(np.abs(x)))),\n        snr_mean_all=(\"snr\", \"mean\"),\n        snr_std_all=(\"snr\", \"std\"),\n        snr_max_all=(\"snr\", \"max\"),\n        pos_frac_all=(\"is_pos\", \"mean\"),\n        time_min_all=(\"Time (MJD)\", \"min\"),\n        time_max_all=(\"Time (MJD)\", \"max\"),\n    ).reset_index()\n    g[\"time_range_all\"] = g[\"time_max_all\"] - g[\"time_min_all\"]\n\n    out = g.merge(feat_pivot, on=\"object_id\", how=\"left\")\n\n    # Cross-filter features (peak time lag + peak flux ratio)\n    eps = 1e-6\n    for a, b in [(\"r\", \"i\"), (\"i\", \"z\"), (\"r\", \"z\"), (\"g\", \"r\")]:\n        ta, tb = f\"peak_time_f{a}\", f\"peak_time_f{b}\"\n        fa, fb = f\"peak_flux_f{a}\", f\"peak_flux_f{b}\"\n        if ta in out.columns and tb in out.columns:\n            out[f\"peak_time_diff_{a}_{b}\"] = out[ta] - out[tb]\n        if fa in out.columns and fb in out.columns:\n            out[f\"peak_flux_ratio_{a}_{b}\"] = (out[fa] + eps) / (out[fb] + eps)\n\n    out = out.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n    return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T19:43:57.553981Z","iopub.execute_input":"2025-12-17T19:43:57.554219Z","iopub.status.idle":"2025-12-17T19:43:57.582588Z","shell.execute_reply.started":"2025-12-17T19:43:57.554202Z","shell.execute_reply":"2025-12-17T19:43:57.581929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 5 – Meta features: Z, EBV, colors, rest-frame times\ndef add_meta_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Add features that depend on meta-data: Z, EBV, colors, rest-frame times.\n    \"\"\"\n    df = df.copy()\n\n    # Add split_id as numeric feature (keep split string dropped later)\n    if \"split\" in df.columns:\n        df[\"split_id\"] = (\n            df[\"split\"].astype(str).str.extract(r\"(\\d+)\")[0].astype(float).fillna(0).astype(int)\n        )\n\n    z = df[\"Z\"].fillna(0.0)\n    ebv = df[\"EBV\"].fillna(0.0)\n    one_plus_z = 1.0 + z\n\n    # Rest-frame durations\n    if \"time_range_all\" in df.columns:\n        df[\"time_range_all_rest\"] = df[\"time_range_all\"] / one_plus_z\n\n    for col in df.columns:\n        if col.startswith(\"time_range_f\"):\n            df[col + \"_rest\"] = df[col] / one_plus_z\n\n    # Simple transforms of meta\n    df[\"log_z_plus1\"] = np.log1p(z)\n    df[\"ebv\"] = ebv\n    df[\"ebv_z\"] = ebv * z\n\n    # Color features from flux_abs_mean per band (proxy for brightness)\n    bands = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]\n    # Approx extinction coefficients A_lambda / E(B-V)\n    R = {\"u\": 4.239, \"g\": 3.303, \"r\": 2.285, \"i\": 1.698, \"z\": 1.263, \"y\": 1.088}\n\n    eps = 1e-6\n    pairs = [(\"u\", \"g\"), (\"g\", \"r\"), (\"r\", \"i\"), (\"i\", \"z\"), (\"z\", \"y\"),\n             (\"g\", \"i\"), (\"g\", \"z\")]\n\n    for b1, b2 in pairs:\n        c1 = f\"flux_abs_mean_f{b1}\"\n        c2 = f\"flux_abs_mean_f{b2}\"\n        if c1 in df.columns and c2 in df.columns:\n            f1 = np.abs(df[c1]) + eps\n            f2 = np.abs(df[c2]) + eps\n            cname = f\"color_{b1}{b2}\"\n            df[cname] = -2.5 * np.log10(f1 / f2)\n\n            # EBV-corrected color\n            delta_R = R[b1] - R[b2]\n            df[cname + \"_deext\"] = df[cname] - ebv * delta_R\n\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T19:43:57.583364Z","iopub.execute_input":"2025-12-17T19:43:57.583667Z","iopub.status.idle":"2025-12-17T19:43:57.602845Z","shell.execute_reply.started":"2025-12-17T19:43:57.583650Z","shell.execute_reply":"2025-12-17T19:43:57.602193Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 6 – Build per-split features + global lightcurves\ntrain_feat_list = []\ntest_feat_list = []\n\nfull_train_lc_list = []\nfull_test_lc_list = []\n\nfor i in range(1, 21):\n    split_name = f\"split_{i:02d}\"\n    print(\"Processing\", split_name)\n\n    lc_tr_path = os.path.join(DATA_DIR, split_name, \"train_full_lightcurves.csv\")\n    lc_te_path = os.path.join(DATA_DIR, split_name, \"test_full_lightcurves.csv\")\n\n    lc_tr = pd.read_csv(lc_tr_path)\n    lc_te = pd.read_csv(lc_te_path)\n\n    full_train_lc_list.append(lc_tr)\n    full_test_lc_list.append(lc_te)\n\n    # Train features\n    tr_feat = make_features(lc_tr)\n    tr_meta = train_log[train_log[\"split\"] == split_name]\n    merged_tr = tr_meta.merge(tr_feat, on=\"object_id\", how=\"left\")\n    merged_tr = add_meta_features(merged_tr)\n    train_feat_list.append(merged_tr)\n\n    # Test features\n    te_feat = make_features(lc_te)\n    te_meta = test_log[test_log[\"split\"] == split_name]\n    merged_te = te_meta.merge(te_feat, on=\"object_id\", how=\"left\")\n    merged_te = add_meta_features(merged_te)\n    test_feat_list.append(merged_te)\n\ntrain_df = pd.concat(train_feat_list, ignore_index=True)\ntest_df  = pd.concat(test_feat_list,  ignore_index=True)\n\nprint(\"train_df:\", train_df.shape)\nprint(\"test_df:\", test_df.shape)\n\nfull_train_lc = pd.concat(full_train_lc_list, ignore_index=True)\nfull_test_lc  = pd.concat(full_test_lc_list,  ignore_index=True)\n\nprint(\"full_train_lc:\", full_train_lc.shape)\nprint(\"full_test_lc:\", full_test_lc.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T19:43:57.603670Z","iopub.execute_input":"2025-12-17T19:43:57.603919Z","iopub.status.idle":"2025-12-17T19:45:07.137573Z","shell.execute_reply.started":"2025-12-17T19:43:57.603878Z","shell.execute_reply":"2025-12-17T19:45:07.136776Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---- NEW: Enrich full lightcurves for corrected + rest-frame tsfresh channels ----\nR_COEFF = {\"u\": 4.239, \"g\": 3.303, \"r\": 2.285, \"i\": 1.698, \"z\": 1.263, \"y\": 1.088}\n\ndef enrich_for_tsfresh(full_lc: pd.DataFrame, log_df: pd.DataFrame) -> pd.DataFrame:\n    lc = full_lc.merge(log_df[[\"object_id\", \"Z\", \"EBV\"]], on=\"object_id\", how=\"left\")\n    lc[\"Z\"] = lc[\"Z\"].fillna(0.0).clip(lower=0.0)\n    lc[\"EBV\"] = lc[\"EBV\"].fillna(0.0)\n\n    # rest-frame time (relative to each object's first obs)\n    t0 = lc.groupby(\"object_id\")[\"Time (MJD)\"].transform(\"min\")\n    lc[\"t_rest\"] = (lc[\"Time (MJD)\"] - t0) / (1.0 + lc[\"Z\"])\n\n    # de-extinction on flux + error\n    A = lc[\"EBV\"] * lc[\"Filter\"].map(R_COEFF).fillna(0.0)\n    fac = np.power(10.0, 0.4 * A)\n\n    lc[\"flux_deext\"] = lc[\"Flux\"] * fac\n    lc[\"fluxerr_deext\"] = lc[\"Flux_err\"] * fac\n    lc[\"snr_deext\"] = lc[\"flux_deext\"] / lc[\"fluxerr_deext\"].replace(0, np.nan)\n    lc[\"snr_deext\"] = lc[\"snr_deext\"].fillna(0.0)\n\n    # stable transform for negatives\n    lc[\"logflux_deext\"] = np.sign(lc[\"flux_deext\"]) * np.log1p(np.abs(lc[\"flux_deext\"]))\n\n    # clipped deext channel (robust)\n    lc[\"flux_deext_clip\"] = lc[\"flux_deext\"].clip(-5, 5)\n\n    return lc\n\nfull_train_lc = enrich_for_tsfresh(full_train_lc, train_log)\nfull_test_lc  = enrich_for_tsfresh(full_test_lc,  test_log)\n\nprint(\"Enriched full_train_lc:\", full_train_lc.shape)\nprint(\"Enriched full_test_lc:\", full_test_lc.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T19:45:07.138412Z","iopub.execute_input":"2025-12-17T19:45:07.138768Z","iopub.status.idle":"2025-12-17T19:45:07.728287Z","shell.execute_reply.started":"2025-12-17T19:45:07.138744Z","shell.execute_reply":"2025-12-17T19:45:07.727536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 7 – tsfresh features (TRAIN) multi-channel + corrected time/flux\n\nfc_params = EfficientFCParameters()\n\ndef build_tsfresh_long(lc: pd.DataFrame) -> pd.DataFrame:\n    parts = []\n    for col, suf in [\n        (\"flux_deext_clip\", \"_flux\"),\n        (\"snr_deext\", \"_snr\"),\n        (\"logflux_deext\", \"_logflux\"),\n    ]:\n        tmp = lc[[\"object_id\", \"t_rest\", \"Filter\", col]].copy()\n        tmp.rename(columns={\"t_rest\": \"time\", col: \"value\"}, inplace=True)\n        tmp[\"kind\"] = tmp[\"Filter\"].astype(str) + suf\n        parts.append(tmp[[\"object_id\", \"time\", \"kind\", \"value\"]])\n\n    ts = pd.concat(parts, ignore_index=True)\n    ts = ts.replace([np.inf, -np.inf], np.nan)\n    ts = ts.dropna(subset=[\"object_id\", \"time\", \"kind\", \"value\"])\n    return ts\n\ntrain_ts = build_tsfresh_long(full_train_lc)\n\nprint(\"train_ts after cleaning:\", train_ts.shape)\n\nX_ts = extract_features(\n    train_ts,\n    column_id=\"object_id\",\n    column_sort=\"time\",\n    column_kind=\"kind\",\n    column_value=\"value\",\n    default_fc_parameters=fc_params,\n    n_jobs=1,\n    disable_progressbar=False,\n)\n\nX_ts = X_ts.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n# Align to train_df order\nX_ts = X_ts.reindex(train_df[\"object_id\"]).fillna(0.0)\n\ny_target = train_df[\"target\"].values\nX_ts_selected = select_features(X_ts, y_target)\n\nprint(\"tsfresh train shape (after selection):\", X_ts_selected.shape)\n\nts_cols = X_ts_selected.columns.tolist()\n\ntrain_df_ts = train_df.merge(\n    X_ts_selected,\n    left_on=\"object_id\",\n    right_index=True,\n    how=\"left\",\n).fillna(0.0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T19:45:07.729114Z","iopub.execute_input":"2025-12-17T19:45:07.729393Z","iopub.status.idle":"2025-12-17T20:54:54.810500Z","shell.execute_reply.started":"2025-12-17T19:45:07.729376Z","shell.execute_reply":"2025-12-17T20:54:54.809511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 8 – tsfresh features (TEST) multi-channel + corrected time/flux\n\ntest_ts = build_tsfresh_long(full_test_lc)\nprint(\"test_ts after cleaning:\", test_ts.shape)\n\nX_ts_test = extract_features(\n    test_ts,\n    column_id=\"object_id\",\n    column_sort=\"time\",\n    column_kind=\"kind\",\n    column_value=\"value\",\n    default_fc_parameters=fc_params,\n    n_jobs=1,\n    disable_progressbar=False,\n)\n\nX_ts_test = X_ts_test.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n# Ensure same columns as train\nmissing_cols = [c for c in ts_cols if c not in X_ts_test.columns]\nfor c in missing_cols:\n    X_ts_test[c] = 0.0\nX_ts_test = X_ts_test[ts_cols]\n\ntest_df_ts = test_df.merge(\n    X_ts_test,\n    left_on=\"object_id\",\n    right_index=True,\n    how=\"left\",\n).fillna(0.0)\n\n# Replace original dfs with enriched versions\ntrain_df = train_df_ts\ntest_df = test_df_ts\n\nprint(\"Final train_df:\", train_df.shape)\nprint(\"Final test_df:\", test_df.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T20:54:54.813398Z","iopub.execute_input":"2025-12-17T20:54:54.813863Z","iopub.status.idle":"2025-12-17T23:37:09.777724Z","shell.execute_reply.started":"2025-12-17T20:54:54.813841Z","shell.execute_reply":"2025-12-17T23:37:09.776830Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 9 – Build X, y, X_test\n\ndrop_cols = [\"object_id\", \"split\", \"SpecType\", \"English Translation\", \"target\"]\n\nfeature_cols = [c for c in train_df.columns if c not in drop_cols]\n\n# Keep NaNs (XGB/LGB handle them); just remove inf\ntrain_df[feature_cols] = train_df[feature_cols].replace([np.inf, -np.inf], np.nan)\ntest_df[feature_cols]  = test_df[feature_cols].replace([np.inf, -np.inf], np.nan)\n\nX      = train_df[feature_cols].astype(np.float32).values\ny      = train_df[\"target\"].values\nX_test = test_df[feature_cols].astype(np.float32).values\n\nprint(\"Number of features:\", len(feature_cols))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T23:37:09.781562Z","iopub.execute_input":"2025-12-17T23:37:09.781744Z","iopub.status.idle":"2025-12-17T23:37:10.375028Z","execution_failed":"2025-12-18T00:19:29.837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 10 training XGBoost + LightGBM + Ensemble (UPDATED: no scaling + GPU XGB + best blend)\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Class imbalance\nn_pos = (y == 1).sum()\nn_neg = (y == 0).sum()\nscale_pos_weight = n_neg / max(n_pos, 1)\nprint(\"Positives:\", n_pos, \"Negatives:\", n_neg, \"scale_pos_weight:\", scale_pos_weight)\n\n# XGBoost (GPU)\nxgb_params = {\n    \"objective\": \"binary:logistic\",\n    \"eval_metric\": \"aucpr\",\n    \"tree_method\": \"gpu_hist\",\n    \"predictor\": \"gpu_predictor\",\n    \"gpu_id\": 0,\n    \"max_depth\": 6,\n    \"eta\": 0.03,\n    \"subsample\": 0.85,\n    \"colsample_bytree\": 0.85,\n    \"lambda\": 2.0,\n    \"alpha\": 0.0,\n    \"min_child_weight\": 5.0,\n    \"gamma\": 0.1,\n    \"scale_pos_weight\": scale_pos_weight,\n    \"seed\": 42,\n}\n\n# LightGBM (CPU)\nlgb_params = {\n    \"objective\": \"binary\",\n    \"metric\": \"average_precision\",\n    \"learning_rate\": 0.035,\n    \"num_leaves\": 120,\n    \"max_depth\": 11,\n    \"feature_fraction\": 0.8,\n    \"bagging_fraction\": 0.8,\n    \"bagging_freq\": 1,\n    \"min_data_in_leaf\": 40,\n    \"lambda_l1\": 0.0,\n    \"lambda_l2\": 2.0,\n    \"scale_pos_weight\": scale_pos_weight,\n    \"verbosity\": -1,\n}\n\noof_pred_xgb = np.zeros(len(train_df))\ntest_pred_xgb = np.zeros(len(test_df))\n\noof_pred_lgb = np.zeros(len(train_df))\ntest_pred_lgb = np.zeros(len(test_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(skf.split(X, y), 1):\n    print(f\"\\n========== Fold {fold} ==========\")\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_tr, y_val = y[tr_idx], y[val_idx]\n\n    # --- XGBoost ---\n    dtrain = xgb.DMatrix(X_tr, label=y_tr, missing=np.nan)\n    dval   = xgb.DMatrix(X_val, label=y_val, missing=np.nan)\n    dtest  = xgb.DMatrix(X_test, missing=np.nan)\n\n    model_xgb = xgb.train(\n        params=xgb_params,\n        dtrain=dtrain,\n        num_boost_round=2500,\n        evals=[(dtrain, \"train\"), (dval, \"valid\")],\n        early_stopping_rounds=200,\n        verbose_eval=200,\n    )\n\n    best_iter_xgb = model_xgb.best_iteration\n    if best_iter_xgb is None:\n        oof_pred_xgb[val_idx] = model_xgb.predict(dval)\n        test_pred_xgb += model_xgb.predict(dtest) / skf.n_splits\n    else:\n        oof_pred_xgb[val_idx] = model_xgb.predict(dval, iteration_range=(0, best_iter_xgb + 1))\n        test_pred_xgb += model_xgb.predict(dtest, iteration_range=(0, best_iter_xgb + 1)) / skf.n_splits\n\n    # --- LightGBM ---\n    lgb_train = lgb.Dataset(X_tr, label=y_tr)\n    lgb_valid = lgb.Dataset(X_val, label=y_val)\n\n    model_lgb = lgb.train(\n        params=lgb_params,\n        train_set=lgb_train,\n        num_boost_round=7000,\n        valid_sets=[lgb_train, lgb_valid],\n        valid_names=[\"train\", \"valid\"],\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=400, first_metric_only=True),\n            lgb.log_evaluation(period=200),\n        ],\n    )\n\n    best_iter_lgb = model_lgb.best_iteration\n    oof_pred_lgb[val_idx] = model_lgb.predict(X_val, num_iteration=best_iter_lgb)\n    test_pred_lgb += model_lgb.predict(X_test, num_iteration=best_iter_lgb) / skf.n_splits\n\n\ndef best_f1_threshold(oof_probs, y_true):\n    thresholds = np.linspace(0.01, 0.99, 99)\n    best_th = 0.5\n    best_f1 = 0.0\n    for th in thresholds:\n        f1 = f1_score(y_true, (oof_probs >= th).astype(int))\n        if f1 > best_f1:\n            best_f1 = f1\n            best_th = th\n    return best_f1, best_th\n\n\n# Evaluate base models\nf1_xgb, th_xgb = best_f1_threshold(oof_pred_xgb, y)\nf1_lgb, th_lgb = best_f1_threshold(oof_pred_lgb, y)\n\nprint(f\"\\n[XGBoost]  Best OOF F1: {f1_xgb:.4f} at threshold={th_xgb:.3f}\")\nprint(f\"[LightGBM] Best OOF F1: {f1_lgb:.4f} at threshold={th_lgb:.3f}\")\n\n# Search best blend weight\nbest_w = 0.5\nbest_f1 = -1.0\nbest_th = 0.5\nfor w in np.linspace(0.0, 1.0, 21):\n    oof_blend = w * oof_pred_xgb + (1 - w) * oof_pred_lgb\n    f1b, thb = best_f1_threshold(oof_blend, y)\n    if f1b > best_f1:\n        best_f1 = f1b\n        best_th = thb\n        best_w = w\n\noof_pred_ens = best_w * oof_pred_xgb + (1 - best_w) * oof_pred_lgb\ntest_pred_ens = best_w * test_pred_xgb + (1 - best_w) * test_pred_lgb\n\nprint(f\"\\n[Blend] Best OOF F1: {best_f1:.4f} at threshold={best_th:.3f} with w={best_w:.2f}*XGB + {1-best_w:.2f}*LGB\")\n\n# Pick best final predictor among XGB/LGB/Blend\nbest_model_name = \"Blend\"\nbest_test_pred = test_pred_ens\n\nif f1_xgb > best_f1:\n    best_model_name = \"XGBoost\"\n    best_f1 = f1_xgb\n    best_th = th_xgb\n    best_test_pred = test_pred_xgb\n\nif f1_lgb > best_f1:\n    best_model_name = \"LightGBM\"\n    best_f1 = f1_lgb\n    best_th = th_lgb\n    best_test_pred = test_pred_lgb\n\nprint(f\"\\n>> Using {best_model_name} with OOF F1 = {best_f1:.4f} and threshold = {best_th:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T04:57:35.749797Z","iopub.execute_input":"2025-12-18T04:57:35.750504Z","iopub.status.idle":"2025-12-18T04:57:35.767914Z","shell.execute_reply.started":"2025-12-18T04:57:35.750469Z","shell.execute_reply":"2025-12-18T04:57:35.766989Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 11 – Build submission.csv\nbinary_prediction = (best_test_pred >= best_th).astype(int)\n\npred_df = pd.DataFrame({\n    \"object_id\": test_df[\"object_id\"],\n    \"prediction\": binary_prediction,\n})\n\nsubmission = sample_sub[[\"object_id\"]].merge(pred_df, on=\"object_id\", how=\"left\")\nsubmission[\"prediction\"] = submission[\"prediction\"].fillna(0).astype(int)\n\nprint(\"Prediction value counts:\")\nprint(submission[\"prediction\"].value_counts())\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Saved submission.csv\")\nsubmission.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T04:57:18.866878Z","iopub.execute_input":"2025-12-18T04:57:18.867479Z","iopub.status.idle":"2025-12-18T04:57:18.877547Z","shell.execute_reply.started":"2025-12-18T04:57:18.867452Z","shell.execute_reply":"2025-12-18T04:57:18.876572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ndef count_pos(p, th):\n    return int((p >= th).sum())\n\nprint(\"\\n==== SUMMARY ====\")\nprint(\"Features:\", len(feature_cols))\nprint(\"Train size:\", len(train_df), \"Pos:\", int((y==1).sum()), \"Neg:\", int((y==0).sum()))\n\nprint(\"\\n==== OOF METRICS ====\")\nprint(f\"XGB  OOF F1={f1_xgb:.5f}  th={th_xgb:.3f}  OOF_pos={count_pos(oof_pred_xgb, th_xgb)}\")\nprint(f\"LGB  OOF F1={f1_lgb:.5f}  th={th_lgb:.3f}  OOF_pos={count_pos(oof_pred_lgb, th_lgb)}\")\nprint(f\"BLND OOF F1={best_f1:.5f} th={best_th:.3f}  OOF_pos={count_pos(oof_pred_ens, best_th)}\")\n\nprint(\"\\n==== TEST PRED COUNTS ====\")\nprint(\"Final model:\", best_model_name)\nprint(\"Test positives predicted:\", count_pos(best_test_pred, best_th), \"out of\", len(best_test_pred))\n\n# Feature importance snapshots (works if you kept models per fold; if not, skip)\ntry:\n    # If you saved last fold models as model_xgb/model_lgb\n    xgb_imp = model_xgb.get_score(importance_type=\"gain\")\n    xgb_imp = pd.Series(xgb_imp).sort_values(ascending=False).head(30)\n    print(\"\\n==== TOP 30 XGB GAIN FEATURES ====\")\n    print(xgb_imp)\n\n    lgb_imp = pd.Series(model_lgb.feature_importance(importance_type=\"gain\"), index=feature_cols)\n    lgb_imp = lgb_imp.sort_values(ascending=False).head(30)\n    print(\"\\n==== TOP 30 LGB GAIN FEATURES ====\")\n    print(lgb_imp)\nexcept Exception as e:\n    print(\"\\n(Feature importance skipped:\", e, \")\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T04:54:24.646477Z","iopub.execute_input":"2025-12-18T04:54:24.646667Z","iopub.status.idle":"2025-12-18T04:54:26.497469Z","shell.execute_reply.started":"2025-12-18T04:54:24.646649Z","shell.execute_reply":"2025-12-18T04:54:26.496527Z"}},"outputs":[],"execution_count":null}]}