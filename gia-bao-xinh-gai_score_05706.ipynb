{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14034160,"sourceType":"datasetVersion","datasetId":8936916}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 0\n!pip install -q tsfresh\n!pip install -q lightgbm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:13:48.089822Z","iopub.execute_input":"2025-12-07T02:13:48.090148Z","iopub.status.idle":"2025-12-07T02:13:57.099002Z","shell.execute_reply.started":"2025-12-07T02:13:48.090123Z","shell.execute_reply":"2025-12-07T02:13:57.097832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 1 – Imports\nimport os\nimport numpy as np\nimport pandas as pd\n\nfrom tsfresh import extract_features, select_features\nfrom tsfresh.feature_extraction import EfficientFCParameters\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, log_loss, f1_score\nfrom sklearn.preprocessing import StandardScaler # Dòng mới\nfrom sklearn.linear_model import LogisticRegression # Dòng mới\nimport xgboost as xgb\nimport lightgbm as lgb # Dòng mới","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:13:57.101236Z","iopub.execute_input":"2025-12-07T02:13:57.101603Z","iopub.status.idle":"2025-12-07T02:14:08.532142Z","shell.execute_reply.started":"2025-12-07T02:13:57.101545Z","shell.execute_reply":"2025-12-07T02:14:08.531219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 2 – Locate DATA_DIR\ncandidate_dirs = [\n    \"/kaggle/input/dataset-macc/dataset_mallorn-astronomical-classification-challenge\"\n]\n\nDATA_DIR = None\nfor d in candidate_dirs:\n    if os.path.exists(os.path.join(d, \"train_log.csv\")):\n        DATA_DIR = d\n        break\n\nif DATA_DIR is None:\n    raise FileNotFoundError(\n        \"Could not find train_log.csv. \"\n        \"Check /kaggle/input and adjust DATA_DIR accordingly.\"\n    )\n\nprint(\"Using DATA_DIR:\", DATA_DIR)\nprint(\"Files:\", os.listdir(DATA_DIR))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:14:08.533097Z","iopub.execute_input":"2025-12-07T02:14:08.533804Z","iopub.status.idle":"2025-12-07T02:14:08.550474Z","shell.execute_reply.started":"2025-12-07T02:14:08.533776Z","shell.execute_reply":"2025-12-07T02:14:08.549636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 3 – Load metadata\ntrain_log = pd.read_csv(os.path.join(DATA_DIR, \"train_log.csv\"))\ntest_log  = pd.read_csv(os.path.join(DATA_DIR, \"test_log.csv\"))\nsample_sub = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n\nprint(\"train_log:\", train_log.shape)\nprint(\"test_log:\", test_log.shape)\nprint(\"sample_submission:\", sample_sub.shape)\n\nprint(\"\\nTarget distribution:\")\nprint(train_log[\"target\"].value_counts(normalize=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:14:08.551362Z","iopub.execute_input":"2025-12-07T02:14:08.551633Z","iopub.status.idle":"2025-12-07T02:14:08.632162Z","shell.execute_reply.started":"2025-12-07T02:14:08.551603Z","shell.execute_reply":"2025-12-07T02:14:08.631340Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4 – Lightcurve feature function\ndef make_features(lc_df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    From lightcurves:\n        object_id, Time (MJD), Flux, Flux_err, Filter\n    -> one row per object_id with per-filter + global stats.\n    \"\"\"\n    lc_df = lc_df.drop_duplicates().copy()\n\n    # Basic per-measurement helpers\n    lc_df[\"snr\"] = lc_df[\"Flux\"] / lc_df[\"Flux_err\"].replace(0, np.nan)\n    lc_df[\"snr\"] = lc_df[\"snr\"].fillna(0.0)\n    lc_df[\"is_pos\"] = (lc_df[\"Flux\"] > 0).astype(float)\n\n    # --- 1) Standard per-band aggregates ---\n    agg = lc_df.groupby([\"object_id\", \"Filter\"]).agg(\n        flux_mean=(\"Flux\", \"mean\"),\n        flux_std=(\"Flux\", \"std\"),\n        flux_min=(\"Flux\", \"min\"),\n        flux_max=(\"Flux\", \"max\"),\n        flux_median=(\"Flux\", \"median\"),\n        flux_count=(\"Flux\", \"count\"),\n        flux_abs_mean=(\"Flux\", lambda x: np.mean(np.abs(x))),\n        time_min=(\"Time (MJD)\", \"min\"),\n        time_max=(\"Time (MJD)\", \"max\"),\n        snr_mean=(\"snr\", \"mean\"),\n        snr_std=(\"snr\", \"std\"),\n        snr_max=(\"snr\", \"max\"),\n        pos_frac=(\"is_pos\", \"mean\"),\n    ).reset_index()\n\n    agg[\"time_range\"] = agg[\"time_max\"] - agg[\"time_min\"]\n    agg[\"flux_amp\"] = agg[\"flux_max\"] - agg[\"flux_min\"]\n    agg[\"obs_rate\"] = agg[\"flux_count\"] / agg[\"time_range\"].replace(0, np.nan)\n    agg[\"obs_rate\"] = agg[\"obs_rate\"].fillna(0.0)\n\n    # --- 2) Extra time-series features per band: slopes + robust amplitude ---\n    def _extra_band_feats(group: pd.DataFrame) -> pd.Series:\n        # group has \"Time (MJD)\" and \"Flux\"\n        t = group[\"Time (MJD)\"].values\n        f = group[\"Flux\"].values\n\n        if len(t) > 1:\n            order = np.argsort(t)\n            t_sorted = t[order]\n            f_sorted = f[order]\n\n            dt = np.diff(t_sorted)\n            df = np.diff(f_sorted)\n            valid = dt > 0\n\n            if np.any(valid):\n                slopes = df[valid] / dt[valid]\n                slope_max = slopes.max()\n                slope_min = slopes.min()\n                slope_mean = slopes.mean()\n                dt_mean = dt.mean()\n            else:\n                slope_max = slope_min = slope_mean = 0.0\n                dt_mean = 0.0\n        else:\n            slope_max = slope_min = slope_mean = 0.0\n            dt_mean = 0.0\n\n        # Robust amplitude\n        if len(f) > 0:\n            flux_p10 = np.percentile(f, 10)\n            flux_p90 = np.percentile(f, 90)\n            flux_p90_p10 = flux_p90 - flux_p10\n        else:\n            flux_p10 = flux_p90 = flux_p90_p10 = 0.0\n\n        return pd.Series(\n            {\n                \"slope_max\": slope_max,\n                \"slope_min\": slope_min,\n                \"slope_mean\": slope_mean,\n                \"dt_mean\": dt_mean,\n                \"flux_p10\": flux_p10,\n                \"flux_p90\": flux_p90,\n                \"flux_p90_p10\": flux_p90_p10,\n            }\n        )\n\n    extra = (\n        lc_df.groupby([\"object_id\", \"Filter\"])[[\"Time (MJD)\", \"Flux\"]]\n        .apply(_extra_band_feats)\n        .reset_index()\n    )\n\n    agg = agg.merge(extra, on=[\"object_id\", \"Filter\"], how=\"left\")\n\n    # Pivot: (stat, band) -> columns like \"flux_mean_fg\", \"slope_max_fr\", ...\n    feat_pivot = agg.pivot(\n        index=\"object_id\",\n        columns=\"Filter\",\n        values=[\n            \"flux_mean\",\n            \"flux_std\",\n            \"flux_min\",\n            \"flux_max\",\n            \"flux_median\",\n            \"flux_count\",\n            \"flux_abs_mean\",\n            \"time_min\",\n            \"time_max\",\n            \"time_range\",\n            \"flux_amp\",\n            \"obs_rate\",\n            \"snr_mean\",\n            \"snr_std\",\n            \"snr_max\",\n            \"pos_frac\",\n            \"slope_max\",\n            \"slope_min\",\n            \"slope_mean\",\n            \"dt_mean\",\n            \"flux_p10\",\n            \"flux_p90\",\n            \"flux_p90_p10\",\n        ],\n    )\n\n    feat_pivot.columns = [f\"{stat}_f{band}\" for stat, band in feat_pivot.columns]\n    feat_pivot = feat_pivot.reset_index()\n\n    # --- 3) Global stats over all filters ---\n    g = lc_df.groupby(\"object_id\").agg(\n        flux_mean_all=(\"Flux\", \"mean\"),\n        flux_std_all=(\"Flux\", \"std\"),\n        flux_min_all=(\"Flux\", \"min\"),\n        flux_max_all=(\"Flux\", \"max\"),\n        flux_median_all=(\"Flux\", \"median\"),\n        flux_count_all=(\"Flux\", \"count\"),\n        flux_abs_mean_all=(\"Flux\", lambda x: np.mean(np.abs(x))),\n        snr_mean_all=(\"snr\", \"mean\"),\n        snr_std_all=(\"snr\", \"std\"),\n        snr_max_all=(\"snr\", \"max\"),\n        pos_frac_all=(\"is_pos\", \"mean\"),\n        time_min_all=(\"Time (MJD)\", \"min\"),\n        time_max_all=(\"Time (MJD)\", \"max\"),\n    ).reset_index()\n\n    g[\"time_range_all\"] = g[\"time_max_all\"] - g[\"time_min_all\"]\n\n    out = g.merge(feat_pivot, on=\"object_id\", how=\"left\")\n\n    # Fill NaNs from std in singleton groups, etc.\n    out = out.fillna(0.0)\n\n    return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:14:08.634652Z","iopub.execute_input":"2025-12-07T02:14:08.634949Z","iopub.status.idle":"2025-12-07T02:14:08.651457Z","shell.execute_reply.started":"2025-12-07T02:14:08.634926Z","shell.execute_reply":"2025-12-07T02:14:08.650352Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 5 – Meta features: Z, EBV, colors, rest-frame times\ndef add_meta_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Add features that depend on meta-data: Z, EBV, colors, rest-frame times.\n    \"\"\"\n    df = df.copy()\n\n    z = df[\"Z\"].fillna(0.0)\n    ebv = df[\"EBV\"].fillna(0.0)\n    one_plus_z = 1.0 + z\n\n    # Rest-frame durations\n    if \"time_range_all\" in df.columns:\n        df[\"time_range_all_rest\"] = df[\"time_range_all\"] / one_plus_z\n\n    for col in df.columns:\n        if col.startswith(\"time_range_f\"):\n            df[col + \"_rest\"] = df[col] / one_plus_z\n\n    # Simple transforms of meta\n    df[\"log_z_plus1\"] = np.log1p(z)\n    df[\"ebv\"] = ebv\n    df[\"ebv_z\"] = ebv * z\n\n    # Color features from flux_abs_mean per band (proxy for brightness)\n    bands = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]\n    # Approx extinction coefficients A_lambda / E(B-V)\n    R = {\"u\": 4.239, \"g\": 3.303, \"r\": 2.285, \"i\": 1.698, \"z\": 1.263, \"y\": 1.088}\n\n    eps = 1e-6\n    pairs = [(\"u\", \"g\"), (\"g\", \"r\"), (\"r\", \"i\"), (\"i\", \"z\"), (\"z\", \"y\"),\n             (\"g\", \"i\"), (\"g\", \"z\")]\n\n    for b1, b2 in pairs:\n        c1 = f\"flux_abs_mean_f{b1}\"\n        c2 = f\"flux_abs_mean_f{b2}\"\n        if c1 in df.columns and c2 in df.columns:\n            f1 = np.abs(df[c1]) + eps\n            f2 = np.abs(df[c2]) + eps\n            cname = f\"color_{b1}{b2}\"\n            df[cname] = -2.5 * np.log10(f1 / f2)\n\n            # EBV-corrected color: color_corr = color_obs - E(B-V)*(R1 - R2)\n            delta_R = R[b1] - R[b2]\n            df[cname + \"_deext\"] = df[cname] - ebv * delta_R\n\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:14:08.652492Z","iopub.execute_input":"2025-12-07T02:14:08.652905Z","iopub.status.idle":"2025-12-07T02:14:08.673971Z","shell.execute_reply.started":"2025-12-07T02:14:08.652862Z","shell.execute_reply":"2025-12-07T02:14:08.673143Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 6 – Build per-split features + global lightcurves\ntrain_feat_list = []\ntest_feat_list = []\n\nfull_train_lc_list = []\nfull_test_lc_list = []\n\nfor i in range(1, 21):\n    split_name = f\"split_{i:02d}\"\n    print(\"Processing\", split_name)\n\n    lc_tr_path = os.path.join(DATA_DIR, split_name, \"train_full_lightcurves.csv\")\n    lc_te_path = os.path.join(DATA_DIR, split_name, \"test_full_lightcurves.csv\")\n\n    lc_tr = pd.read_csv(lc_tr_path)\n    lc_te = pd.read_csv(lc_te_path)\n\n    full_train_lc_list.append(lc_tr)\n    full_test_lc_list.append(lc_te)\n\n    # Train features\n    tr_feat = make_features(lc_tr)\n    tr_meta = train_log[train_log[\"split\"] == split_name]\n    merged_tr = tr_meta.merge(tr_feat, on=\"object_id\", how=\"left\")\n    merged_tr = add_meta_features(merged_tr)\n    train_feat_list.append(merged_tr)\n\n    # Test features\n    te_feat = make_features(lc_te)\n    te_meta = test_log[test_log[\"split\"] == split_name]\n    merged_te = te_meta.merge(te_feat, on=\"object_id\", how=\"left\")\n    merged_te = add_meta_features(merged_te)\n    test_feat_list.append(merged_te)\n\ntrain_df = pd.concat(train_feat_list, ignore_index=True)\ntest_df  = pd.concat(test_feat_list,  ignore_index=True)\n\nprint(\"train_df:\", train_df.shape)\nprint(\"test_df:\", test_df.shape)\n\nfull_train_lc = pd.concat(full_train_lc_list, ignore_index=True)\nfull_test_lc  = pd.concat(full_test_lc_list,  ignore_index=True)\n\nprint(\"full_train_lc:\", full_train_lc.shape)\nprint(\"full_test_lc:\", full_test_lc.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:14:08.674983Z","iopub.execute_input":"2025-12-07T02:14:08.675283Z","iopub.status.idle":"2025-12-07T02:14:57.276489Z","shell.execute_reply.started":"2025-12-07T02:14:08.675224Z","shell.execute_reply":"2025-12-07T02:14:57.275722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 7 – tsfresh features (TRAIN) with NaN cleaning\n\n# === tsfresh feature extraction: TRAIN ===\n\nfc_params = EfficientFCParameters()\n\ntrain_ts = full_train_lc[[\"object_id\", \"Time (MJD)\", \"Filter\", \"Flux\"]].copy()\ntrain_ts.rename(columns={\"Time (MJD)\": \"time\", \"Flux\": \"value\"}, inplace=True)\n\n# Clean up before feeding into tsfresh: no NaNs or inf allowed in id/kind/value/time\ntrain_ts = train_ts.replace([np.inf, -np.inf], np.nan)\ntrain_ts = train_ts.dropna(subset=[\"object_id\", \"Filter\", \"time\", \"value\"])\n\nprint(\"train_ts after cleaning:\", train_ts.shape)\nprint(\"Any NaNs left in value?\", train_ts[\"value\"].isna().any())\n\nX_ts = extract_features(\n    train_ts,\n    column_id=\"object_id\",\n    column_sort=\"time\",\n    column_kind=\"Filter\",\n    column_value=\"value\",\n    default_fc_parameters=fc_params,\n    n_jobs=4,\n    disable_progressbar=False,\n)\n\n# Clean infinities / NaNs coming from feature computations\nX_ts = X_ts.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n# Align to train_df order (by object_id)\nX_ts = X_ts.reindex(train_df[\"object_id\"]).fillna(0.0)\n\ny_target = train_df[\"target\"].values\nX_ts_selected = select_features(X_ts, y_target)\n\nprint(\"tsfresh train shape (after selection):\", X_ts_selected.shape)\n\nts_cols = X_ts_selected.columns.tolist()\n\ntrain_df_ts = train_df.merge(\n    X_ts_selected,\n    left_on=\"object_id\",\n    right_index=True,\n    how=\"left\",\n)\n\ntrain_df_ts = train_df_ts.fillna(0.0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:14:57.277526Z","iopub.execute_input":"2025-12-07T02:14:57.277897Z","iopub.status.idle":"2025-12-07T02:26:30.986416Z","shell.execute_reply.started":"2025-12-07T02:14:57.277864Z","shell.execute_reply":"2025-12-07T02:26:30.985362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 8 – tsfresh features (TEST) with NaN cleaning\n\n# === tsfresh feature extraction: TEST ===\n\ntest_ts = full_test_lc[[\"object_id\", \"Time (MJD)\", \"Filter\", \"Flux\"]].copy()\ntest_ts.rename(columns={\"Time (MJD)\": \"time\", \"Flux\": \"value\"}, inplace=True)\n\n# Clean up before tsfresh\ntest_ts = test_ts.replace([np.inf, -np.inf], np.nan)\ntest_ts = test_ts.dropna(subset=[\"object_id\", \"Filter\", \"time\", \"value\"])\n\nprint(\"test_ts after cleaning:\", test_ts.shape)\nprint(\"Any NaNs left in value?\", test_ts[\"value\"].isna().any())\n\nX_ts_test = extract_features(\n    test_ts,\n    column_id=\"object_id\",\n    column_sort=\"time\",\n    column_kind=\"Filter\",\n    column_value=\"value\",\n    default_fc_parameters=fc_params,\n    n_jobs=4,\n    disable_progressbar=False,\n)\n\nX_ts_test = X_ts_test.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\n# Ensure same columns as in train\nmissing_cols = [c for c in ts_cols if c not in X_ts_test.columns]\nfor c in missing_cols:\n    X_ts_test[c] = 0.0\n\nX_ts_test = X_ts_test[ts_cols]\n\ntest_df_ts = test_df.merge(\n    X_ts_test,\n    left_on=\"object_id\",\n    right_index=True,\n    how=\"left\",\n)\n\ntest_df_ts = test_df_ts.fillna(0.0)\n\n# Replace original dfs with enriched versions\ntrain_df = train_df_ts\ntest_df = test_df_ts\n\nprint(\"Final train_df:\", train_df.shape)\nprint(\"Final test_df:\", test_df.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:26:30.987911Z","iopub.execute_input":"2025-12-07T02:26:30.988519Z","iopub.status.idle":"2025-12-07T02:53:11.100422Z","shell.execute_reply.started":"2025-12-07T02:26:30.988492Z","shell.execute_reply":"2025-12-07T02:53:11.099454Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 9 – Build X, y, X_test\n\ndrop_cols = [\"object_id\", \"split\", \"SpecType\", \"English Translation\", \"target\"]\n\nfeature_cols = [c for c in train_df.columns if c not in drop_cols]\n\ntrain_df[feature_cols] = train_df[feature_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0)\ntest_df[feature_cols]  = test_df[feature_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n\nX      = train_df[feature_cols].values\ny      = train_df[\"target\"].values\nX_test = test_df[feature_cols].values\n\nprint(\"Number of features:\", len(feature_cols))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:53:11.101659Z","iopub.execute_input":"2025-12-07T02:53:11.101933Z","iopub.status.idle":"2025-12-07T02:53:11.412778Z","shell.execute_reply.started":"2025-12-07T02:53:11.101912Z","shell.execute_reply":"2025-12-07T02:53:11.411866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 10 training XGBoost + LightGBM + Ensemble\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale features (đối với LightGBM; XGBoost không cần nhưng scaling cũng không gây hại)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_test_scaled = scaler.transform(X_test)\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Containers cho mỗi mô hình\noof_pred_xgb = np.zeros(len(train_df))\ntest_pred_xgb = np.zeros(len(test_df))\n\noof_pred_lgb = np.zeros(len(train_df))\ntest_pred_lgb = np.zeros(len(test_df))\n\n# Xử lý mất cân bằng lớp\nn_pos = (y == 1).sum()\nn_neg = (y == 0).sum()\nscale_pos_weight = n_neg / n_pos\nprint(\"Positives:\", n_pos, \"Negatives:\", n_neg, \"scale_pos_weight:\", scale_pos_weight)\n\n# --- Tham số XGBoost (kiểu tương tự mô hình tốt nhất của bạn) ---\nxgb_params = {\n    \"objective\": \"binary:logistic\",\n    \"eval_metric\": \"aucpr\",\n    \"tree_method\": \"hist\",\n    \"max_depth\": 5,\n    \"eta\": 0.03,\n    \"subsample\": 0.8,\n    \"colsample_bytree\": 0.8,\n    \"lambda\": 2.0,\n    \"alpha\": 0.0,\n    \"min_child_weight\": 5.0,\n    \"gamma\": 0.1,\n    \"scale_pos_weight\": scale_pos_weight,\n}\n\n# --- Tham số LightGBM (lấy cảm hứng từ giải pháp hạng 7) ---\nlgb_params = {\n    \"objective\": \"binary\",\n    # Tên của PR-AUC trong LightGBM là \"average_precision\"\n    \"metric\": \"average_precision\",\n    \"learning_rate\": 0.0361,\n    \"num_leaves\": 120,\n    \"max_depth\": 11,\n    \"feature_fraction\": 0.8,\n    \"bagging_fraction\": 0.8,\n    \"bagging_freq\": 1,\n    \"min_data_in_leaf\": 40,\n    \"lambda_l1\": 0.0,\n    \"lambda_l2\": 2.0,\n    \"scale_pos_weight\": scale_pos_weight,\n    \"verbosity\": -1,  # \"verbosity\" là key được ưa dùng hơn; \"verbose\" cũng được nhưng cái này gọn hơn\n}\n\nfor fold, (tr_idx, val_idx) in enumerate(skf.split(X, y), 1):\n    print(f\"\\n========== Fold {fold} ==========\")\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_tr, y_val = y[tr_idx], y[val_idx]\n\n    # ---------- XGBoost ----------\n    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n    dval   = xgb.DMatrix(X_val, label=y_val)\n    dtest  = xgb.DMatrix(X_test)\n\n    evals = [(dtrain, \"train\"), (dval, \"valid\")]\n\n    model_xgb = xgb.train(\n        params=xgb_params,\n        dtrain=dtrain,\n        num_boost_round=2000,\n        evals=evals,\n        early_stopping_rounds=200,\n        verbose_eval=100,\n    )\n\n    best_iter_xgb = model_xgb.best_iteration\n    if best_iter_xgb is None:\n        oof_pred_xgb[val_idx] = model_xgb.predict(dval)\n        test_pred_xgb += model_xgb.predict(dtest) / skf.n_splits\n    else:\n        oof_pred_xgb[val_idx] = model_xgb.predict(dval, iteration_range=(0, best_iter_xgb + 1))\n        test_pred_xgb += model_xgb.predict(dtest, iteration_range=(0, best_iter_xgb + 1)) / skf.n_splits\n\n    # ---------- LightGBM ----------\n    X_tr_s = X_scaled[tr_idx]\n    X_val_s = X_scaled[val_idx]\n\n    lgb_train = lgb.Dataset(X_tr_s, label=y_tr)\n    lgb_valid = lgb.Dataset(X_val_s, label=y_val, reference=lgb_train)\n\n    model_lgb = lgb.train(\n        params=lgb_params,\n        train_set=lgb_train,\n        num_boost_round=5000,\n        valid_sets=[lgb_train, lgb_valid],\n        valid_names=[\"train\", \"valid\"],\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=300, first_metric_only=True),\n            lgb.log_evaluation(period=100),\n        ],\n    )\n\n    best_iter_lgb = model_lgb.best_iteration\n    oof_pred_lgb[val_idx] = model_lgb.predict(X_val_s, num_iteration=best_iter_lgb)\n    test_pred_lgb += model_lgb.predict(X_test_scaled, num_iteration=best_iter_lgb) / skf.n_splits\n\n# ---------- Ensemble (trung bình đơn giản) ----------\noof_pred_ens = 0.5 * oof_pred_xgb + 0.5 * oof_pred_lgb\ntest_pred_ens = 0.5 * test_pred_xgb + 0.5 * test_pred_lgb\n\n\n# ---------- Đánh giá từng mô hình và chọn mô hình tốt nhất bằng OOF F1 ----------\ndef find_best_f1(oof_probs, y_true, name):\n    oof_clip = np.clip(oof_probs, 1e-6, 1 - 1e-6)\n    auc = roc_auc_score(y_true, oof_clip)\n    ll  = log_loss(y_true, oof_clip)\n    print(f\"\\n[{name}] OOF ROC AUC: {auc:.4f}, logloss: {ll:.4f}\")\n\n    thresholds = np.linspace(0.01, 0.99, 99)\n    best_th = 0.5\n    best_f1 = 0.0\n    for th in thresholds:\n        preds_bin = (oof_probs >= th).astype(int)\n        f1 = f1_score(y_true, preds_bin)\n        if f1 > best_f1:\n            best_f1 = f1\n            best_th = th\n    print(f\"[{name}] Best OOF F1: {best_f1:.4f} at threshold = {best_th:.3f}\")\n    return best_f1, best_th\n\nf1_xgb, th_xgb = find_best_f1(oof_pred_xgb, y, \"XGBoost\")\nf1_lgb, th_lgb = find_best_f1(oof_pred_lgb, y, \"LightGBM\")\nf1_ens, th_ens = find_best_f1(oof_pred_ens, y, \"Ensemble\")\n\nbest_model_name = \"XGBoost\"\nbest_f1 = f1_xgb\nbest_th = th_xgb\nbest_test_pred = test_pred_xgb\n\nif f1_lgb > best_f1:\n    best_model_name = \"LightGBM\"\n    best_f1 = f1_lgb\n    best_th = th_lgb\n    best_test_pred = test_pred_lgb\n\nif f1_ens > best_f1:\n    best_model_name = \"Ensemble\"\n    best_f1 = f1_ens\n    best_th = th_ens\n    best_test_pred = test_pred_ens\n\nprint(f\"\\n>> Using {best_model_name} with OOF F1 = {best_f1:.4f} and threshold = {best_th:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:53:11.413860Z","iopub.execute_input":"2025-12-07T02:53:11.414090Z","iopub.status.idle":"2025-12-07T02:54:35.580557Z","shell.execute_reply.started":"2025-12-07T02:53:11.414072Z","shell.execute_reply":"2025-12-07T02:54:35.579629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 11 – Build submission.csv\nbinary_prediction = (best_test_pred >= best_th).astype(int)\n\npred_df = pd.DataFrame({\n    \"object_id\": test_df[\"object_id\"],\n    \"prediction\": binary_prediction,\n})\n\nsubmission = sample_sub[[\"object_id\"]].merge(pred_df, on=\"object_id\", how=\"left\")\nsubmission[\"prediction\"] = submission[\"prediction\"].fillna(0).astype(int)\n\nprint(\"Prediction value counts:\")\nprint(submission[\"prediction\"].value_counts())\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Saved submission.csv\")\nsubmission.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T02:54:35.581648Z","iopub.execute_input":"2025-12-07T02:54:35.581957Z","iopub.status.idle":"2025-12-07T02:54:35.628904Z","shell.execute_reply.started":"2025-12-07T02:54:35.581932Z","shell.execute_reply":"2025-12-07T02:54:35.628202Z"}},"outputs":[],"execution_count":null}]}